import logging
import os
from typing import Dict, Any

import torch
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader, random_split
from torch_geometric.loader import DataLoader

from ..data import MeshSimplificationDataset
from ..losses import CombinedMeshSimplificationLoss
from ..metrics import chamfer_distance, normal_consistency, edge_preservation, hausdorff_distance
from ..models import NeuralMeshSimplification


class Trainer:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = NeuralMeshSimplification(
            input_dim=config["model"]["input_dim"],
            hidden_dim=config["model"]["hidden_dim"],
            num_layers=config["model"]["num_layers"],
            k=config["model"]["k"],
            edge_k=config["model"]["edge_k"],
        ).to(self.device)
        self.optimizer = Adam(
            self.model.parameters(), lr=config["training"]["learning_rate"]
        )
        self.scheduler = ReduceLROnPlateau(
            self.optimizer, mode="min", factor=0.1, patience=10, verbose=True
        )
        self.criterion = CombinedMeshSimplificationLoss(
            lambda_c=config["loss"]["lambda_c"],
            lambda_e=config["loss"]["lambda_e"],
            lambda_o=config["loss"]["lambda_o"],
        )
        self.early_stopping_patience = config["training"]["early_stopping_patience"]
        self.best_val_loss = float("inf")
        self.early_stopping_counter = 0
        self.checkpoint_dir = config["training"]["checkpoint_dir"]
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        self.train_loader, self.val_loader = self._prepare_data_loaders()

    def _prepare_data_loaders(self):
        dataset = MeshSimplificationDataset(data_dir=self.config["data"]["data_dir"])
        val_size = int(len(dataset) * self.config["data"]["val_split"])
        train_size = len(dataset) - val_size
        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config["training"]["batch_size"],
            shuffle=True,
            num_workers=self.config["data"]["num_workers"],
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config["training"]["batch_size"],
            shuffle=False,
            num_workers=self.config["data"]["num_workers"],
        )
        return train_loader, val_loader

    def train(self):
        for epoch in range(self.config["training"]["num_epochs"]):
            self._train_one_epoch(epoch)
            val_loss = self._validate(epoch)
            self.scheduler.step(val_loss)
            self._save_checkpoint(epoch, val_loss)
            if self._early_stopping(val_loss):
                logging.info("Early stopping triggered.")
                break

    def _train_one_epoch(self, epoch: int):
        self.model.train()
        running_loss = 0.0
        for batch in self.train_loader:
            self.optimizer.zero_grad()
            batch = batch.to(self.device)
            output = self.model(batch)
            loss = self.criterion(batch, output)
            loss.backward()
            self.optimizer.step()
            running_loss += loss.item()
        logging.info(
            f"Epoch [{epoch + 1}/{self.config['training']['num_epochs']}], Loss: {running_loss / len(self.train_loader)}")

    def _validate(self, epoch: int) -> float:
        self.model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch in self.val_loader:
                batch = batch.to(self.device)
                output = self.model(batch)
                loss = self.criterion(batch, output)
                val_loss += loss.item()
        val_loss /= len(self.val_loader)
        logging.info(f"Epoch [{epoch + 1}/{self.config['training']['num_epochs']}], Validation Loss: {val_loss}")
        return val_loss

    def _save_checkpoint(self, epoch: int, val_loss: float):
        checkpoint_path = os.path.join(self.checkpoint_dir, f"checkpoint_epoch_{epoch + 1}.pth")
        torch.save(
            {
                "epoch": epoch + 1,
                "model_state_dict": self.model.state_dict(),
                "optimizer_state_dict": self.optimizer.state_dict(),
                "val_loss": val_loss,
            },
            checkpoint_path,
        )
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            best_model_path = os.path.join(self.checkpoint_dir, "best_model.pth")
            torch.save(self.model.state_dict(), best_model_path)

    def _early_stopping(self, val_loss: float) -> bool:
        if val_loss < self.best_val_loss:
            self.early_stopping_counter = 0
        else:
            self.early_stopping_counter += 1
        return self.early_stopping_counter >= self.early_stopping_patience

    def load_checkpoint(self, checkpoint_path: str):
        checkpoint = torch.load(checkpoint_path)
        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        self.best_val_loss = checkpoint["val_loss"]
        logging.info(f"Loaded checkpoint from {checkpoint_path} (epoch {checkpoint['epoch']})")

    def log_metrics(self, metrics: Dict[str, float], epoch: int):
        log_message = f"Epoch [{epoch + 1}/{self.config['training']['num_epochs']}], "
        log_message += ", ".join([f"{key}: {value:.4f}" for key, value in metrics.items()])
        logging.info(log_message)

    def evaluate(self, data_loader: DataLoader) -> Dict[str, float]:
        self.model.eval()
        metrics = {"chamfer_distance": 0.0, "normal_consistency": 0.0, "edge_preservation": 0.0,
                   "hausdorff_distance": 0.0}
        with torch.no_grad():
            for batch in data_loader:
                batch = batch.to(self.device)
                output = self.model(batch)
                metrics["chamfer_distance"] += chamfer_distance(batch, output)
                metrics["normal_consistency"] += normal_consistency(batch, output)
                metrics["edge_preservation"] += edge_preservation(batch, output)
                metrics["hausdorff_distance"] += hausdorff_distance(batch, output)
        for key in metrics:
            metrics[key] /= len(data_loader)
        return metrics

    def handle_error(self, error: Exception):
        logging.error(f"An error occurred: {error}")
        if isinstance(error, RuntimeError) and "out of memory" in str(error):
            logging.error("Out of memory error. Attempting to recover.")
            torch.cuda.empty_cache()
        else:
            raise error

    def save_training_state(self, state_path: str):
        torch.save(
            {
                "model_state_dict": self.model.state_dict(),
                "optimizer_state_dict": self.optimizer.state_dict(),
                "best_val_loss": self.best_val_loss,
                "early_stopping_counter": self.early_stopping_counter,
            },
            state_path,
        )

    def load_training_state(self, state_path: str):
        state = torch.load(state_path)
        self.model.load_state_dict(state["model_state_dict"])
        self.optimizer.load_state_dict(state["optimizer_state_dict"])
        self.best_val_loss = state["best_val_loss"]
        self.early_stopping_counter = state["early_stopping_counter"]
        logging.info(f"Loaded training state from {state_path}")
